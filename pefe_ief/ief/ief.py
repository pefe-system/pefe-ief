from numpy import ndarray
from pathlib import Path
import os
import re
import gc
from copy import deepcopy
from typing import Callable, Type, Dict, Union

from ..models.abstract_model import AbstractModel
from ..metrics.simple_stats import stats_per_thresholds
from ..metrics.curves.roc_and_friends import roc_and_friends

ResultEntry = Dict[str, Dict[str, Union[int, float]]]

import msgpack
import msgpack_numpy
msgpack_numpy.patch()

def clean_filename(filename):
    # Generated by Gemini
    # Define a pattern for common illegal characters in Windows and Linux
    # This pattern excludes alphanumeric characters, underscores, hyphens, and periods
    illegal_chars_pattern = r'[<>:"/\\|?*\x00-\x1F]'
    cleaned_filename = re.sub(illegal_chars_pattern, '_', filename)
    return cleaned_filename

def alphanumeric_sort_key(filename: str):
    # so that we get epoch_2 before epoch_10, not the other way around
    maTch = re.search(r'(\d+)', filename)
    num = -1
    if maTch:
        num = int(maTch.group(1))
    return (num, filename)



class IEF:
    class EvaluationConfig:
        def __init__(self, thresholds=[0.5]):
            # type: (IEF.EvaluationConfig, list[float]) -> None
            self.thresholds = deepcopy(thresholds)



    def __init__(self, results_dir):
        # type: (IEF, str) -> None

        assert os.path.isdir(results_dir), f"Results directory does not exist: {results_dir}"

        self.results_dir = results_dir
        self.images_dir_path = Path(self.results_dir) / "images"
        os.makedirs(self.images_dir_path, exist_ok=True)

    def run(
        self,
        models_dirs_and_classes,
        is_model_checkpoint_file,
        get_model_checkpoint_name,
        X_test, y_test,
        config=EvaluationConfig()
    ):
        # type: (IEF,  dict[str, Type[AbstractModel]],    Callable[[Type[AbstractModel], str], bool],    Callable[[Type[AbstractModel], str, str], str|None],    ndarray, ndarray,    IEF.EvaluationConfig)            -> list[ResultEntry]

        INDEX_FILE_PATH = Path(self.results_dir) / "index.msgpack"
        index_file = open(INDEX_FILE_PATH, 'wb')
        try:
            RESULTS = self.evaluate_all(
                models_dirs_and_classes=models_dirs_and_classes,
                is_model_checkpoint_file=is_model_checkpoint_file,
                get_model_checkpoint_name=get_model_checkpoint_name,
                X_test=X_test, y_test=y_test, config=config
            )
            msgpack.pack(RESULTS, index_file, use_bin_type=True)
            print()
            print(f"+++++++++++++++++++++++++++++++++++")
            print(f"Done. Numerical results and metadata have been saved to:")
            print(f"    {os.path.abspath(str(INDEX_FILE_PATH))}")
            print(f"In the IEF notebook, set the RESULTS_DIR variable to:")
            print(f"    {str(self.results_dir)}")
            print(f"+++++++++++++++++++++++++++++++++++")
        finally:
            index_file.close()
    
    MODEL_NAME_REGEX = re.compile(r'^[A-Za-z0-9_.,+=-]+$')
    
    def evaluate_all(
        self,
        models_dirs_and_classes,
        is_model_checkpoint_file,
        get_model_checkpoint_name,
        X_test, y_test,
        config=EvaluationConfig()
    ):
        # type: (IEF,  dict[str, Type[AbstractModel]],    Callable[[Type[AbstractModel], str], bool],    Callable[[Type[AbstractModel], str, str], str|None],    ndarray, ndarray,    IEF.EvaluationConfig)            -> list[ResultEntry]
        """
        Discovers all model checkpoints given
        and evaluates them all.
        Returns an array of result entries to be
        packed to the IEF metadata file.

        You must specify a filter callback which
        takes the model class and a file path,
        determines whether that file is a model
        checkpoint that belongs to that model
        class (return True) or not (return False).

        You must also specify a callback that,
        given the model class, class name, and a
        model's checkpoint file path, returns the
        model [checkpoint]'s name to be displayed
        on plots and reports. The name must match
        the regex `IEF.MODEL_NAME_REGEX`. Alternatively,
        you might also return None so that the
        model name would be generated automatically
        from model class name and the checkpoint
        file's absolute path, but that means insanely
        long and irritating names!
        """
        RESULTS = [] # type: list[ResultEntry]

        def ief_on_checkpoint(model_class, checkpoint_path):
            # type: (Type[AbstractModel], str) -> ResultEntry

            MODEL_TYPE_NAME = model_class.__name__
            MODEL_PATH = os.path.abspath(checkpoint_path)
            MODEL_NAME = get_model_checkpoint_name(model_class, MODEL_TYPE_NAME, MODEL_PATH)
            if MODEL_NAME is None:
                MODEL_NAME = MODEL_TYPE_NAME + "+" + MODEL_PATH
            MODEL_NAME = clean_filename(MODEL_NAME)

            assert self.MODEL_NAME_REGEX.match(MODEL_NAME) is not None, "Model name does not match regex: " + MODEL_NAME

            model = model_class()
            model.load(MODEL_PATH)
            result = self.evaluate(model, MODEL_NAME, X_test, y_test, config)
            del model
            gc.collect()

            result["MODEL"] = {
                "type": MODEL_TYPE_NAME,
                "path": MODEL_PATH,
                "name": MODEL_NAME,
            }

            return result
        
        for models_dir_path, model_class in models_dirs_and_classes.items():
            for current_dir, dirnames, filenames in os.walk(models_dir_path, followlinks=True):
                dirnames.sort(key=alphanumeric_sort_key)
                filenames.sort(key=alphanumeric_sort_key)
                for filename in filenames:
                    filepath = os.path.join(current_dir, filename)
                    if is_model_checkpoint_file(model_class, filepath):
                        RESULTS.append(
                            ief_on_checkpoint(model_class, filepath)
                        )
        
        return RESULTS
    
    def evaluate(
        self, model, model_checkpoint_name,
        X_test, y_test,
        config=EvaluationConfig()
    ):
        # type: (IEF, AbstractModel, str, ndarray, ndarray, IEF.EvaluationConfig) -> dict[str, dict[str, int | float]]
        """
        Returns a single entry of the result of evaluating
        the given model.

        The model must be loaded in prior and ready for
        inference.
        """
        print("Evaluating model: " + model_checkpoint_name)
        y_probs = model.predict(X_test)

        figure_image_file_prefix = str(self.images_dir_path / model_checkpoint_name)
        
        RESULT = stats_per_thresholds(y_probs, y_test, config.thresholds)

        roc_and_friends_evaluation = roc_and_friends(
            figure_image_file_prefix, y_test, y_probs,
            autoshow=False, autosave=True,
        )
        RESULT["curves"] = roc_and_friends_evaluation["curves"]
        RESULT["common_stats"] = {}
        RESULT["common_stats"].update(
            roc_and_friends_evaluation["stats"]
        )

        return RESULT
